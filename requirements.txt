# üöÄ govbr-data-pipeline

![Status](https://img.shields.io/badge/status-em%20desenvolvimento-yellow)
![Python Version](https://img.shields.io/badge/python-3.9+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

## üìñ Sobre o Projeto

Este projeto implementa um pipeline de dados end-to-end para coletar, processar, armazenar e analisar dados de licita√ß√µes e contratos p√∫blicos do Governo Federal do Brasil. O objetivo √© criar uma fonte de dados consolidada e confi√°vel (Data Warehouse) para extrair insights anal√≠ticos sobre os gastos p√∫blicos.

Este reposit√≥rio serve como um projeto de portf√≥lio, demonstrando habilidades em Engenharia de Dados, Arquitetura de Nuvem na AWS e automa√ß√£o de processos.

---

## üìä Status do Projeto

- [x] **Fase 1: Ingest√£o (Camada Bronze)** - Coleta de dados brutos e armazenamento no Data Lake.
- [ ] **Fase 2: Transforma√ß√£o (Camada Prata)** - Limpeza e padroniza√ß√£o dos dados com AWS Glue.
- [ ] **Fase 3: Modelagem e Carga (Camada Ouro)** - Carga dos dados no Data Warehouse (Redshift).
- [ ] **Fase 4: An√°lise e Visualiza√ß√£o** - Cria√ß√£o de consultas anal√≠ticas.
- [ ] **Fase 5: Automa√ß√£o e Deploy** - Orquestra√ß√£o com Step Functions e IaC com Terraform.

---

## üèõÔ∏è Diagrama da Arquitetura

*Obs: O diagrama final ser√° inserido aqui. Ferramenta sugerida: [draw.io](https://app.diagrams.net/)*

O fluxo de dados segue a arquitetura abaixo:

![Diagrama da Arquitetura](https://via.placeholder.com/800x400.png?text=WIP%3A+Diagrama+da+Arquitetura+do+Pipeline)

* **Fontes de Dados:** Portais do Governo Federal (ex: Portal da Transpar√™ncia).
* **Ingest√£o:** Um scraper em Python coleta os dados e os salva em formato JSON no **AWS S3** (Data Lake - Camada Bronze).
* **ETL:** O **AWS Glue** processa os dados brutos, limpando-os e convertendo para Parquet na Camada Prata.
* **Data Warehouse:** O **AWS Redshift** armazena os dados modelados (Star Schema) na Camada Ouro para consultas de alta performance.
* **An√°lise:** Ferramentas de BI e clientes SQL consomem os dados do Redshift.
* **Orquestra√ß√£o:** O **AWS Step Functions** gerencia o fluxo de execu√ß√£o do pipeline.
* **Infraestrutura:** O **Terraform** provisiona e gerencia todos os recursos na nuvem.

---

## üõ†Ô∏è Tecnologias Utilizadas

- **Linguagem:**
  ![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
- **Cloud (AWS):**
  ![AWS](https://img.shields.io/badge/AWS-%23FF9900.svg?style=for-the-badge&logo=amazon-aws&logoColor=white)
  ![S3](https://img.shields.io/badge/S3-569A31?style=for-the-badge&logo=amazon-s3&logoColor=white)
  ![Glue](https://img.shields.io/badge/Glue-4D5C7F?style=for-the-badge&logo=aws-glue&logoColor=white)
  ![Redshift](https://img.shields.io/badge/Redshift-8C4785?style=for-the-badge&logo=amazon-redshift&logoColor=white)
  ![Step Functions](https://img.shields.io/badge/Step%20Functions-D52441?style=for-the-badge&logo=aws-step-functions&logoColor=white)
- **Infraestrutura como C√≥digo:**
  ![Terraform](https://img.shields.io/badge/Terraform-7B42BC?style=for-the-badge&logo=terraform&logoColor=white)
- **Bibliotecas Python:**
  - Scrapy / Requests
  - Boto3
  - Pandas (para explora√ß√£o)

---

## ‚öôÔ∏è Configura√ß√£o e Instala√ß√£o

Siga os passos abaixo para configurar o ambiente de desenvolvimento local.

**Pr√©-requisitos:**
* Conta na AWS com credenciais configuradas localmente.
* Python 3.9+
* Terraform (para as fases futuras)

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone [https://github.com/seu-usuario/govbr-data-pipeline.git](https://github.com/seu-usuario/govbr-data-pipeline.git)
    cd govbr-data-pipeline
    ```

2.  **Crie e ative um ambiente virtual:**
    ```bash
    python -m venv .venv
    # Windows
    .\.venv\Scripts\activate
    # macOS / Linux
    source .venv/bin/activate
    ```

3.  **Instale as depend√™ncias:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configure suas credenciais:**
    * **M√©todo 1 (Recomendado):** Use o AWS CLI.
        ```bash
        aws configure
        ```
    * **M√©todo 2:** Crie um arquivo `.env` na raiz do projeto (ele ser√° ignorado pelo Git) e adicione suas chaves:
        ```
        # .env
        AWS_ACCESS_KEY_ID=SUA_CHAVE_DE_ACESSO
        AWS_SECRET_ACCESS_KEY=SUA_CHAVE_SECRETA
        AWS_REGION=sua-regiao
        ```

---

## üöÄ Como Executar

### Fase 1: Ingest√£o de Dados

Para executar o script de ingest√£o e salvar os dados no S3:

1.  Navegue at√© a pasta de ingest√£o:
    ```bash
    cd src/ingestion
    ```

2.  Execute o script principal do scraper:
    *(Exemplo - ajuste conforme o nome do seu script)*
    ```bash
    python scraper_principal.py --bucket_name "seu-bucket-de-dados-bronze"
    ```

---

## üìÅ Estrutura do Projeto
/govbr-data-pipeline/
|
‚îú‚îÄ‚îÄ .gitignore             # Ignora arquivos desnecess√°rios
‚îú‚îÄ‚îÄ README.md              # Documenta√ß√£o do projeto
‚îú‚îÄ‚îÄ requirements.txt       # Depend√™ncias Python
|
‚îú‚îÄ‚îÄ infra/                 # C√≥digo Terraform (IaC)
|
‚îú‚îÄ‚îÄ notebooks/             # Notebooks para explora√ß√£o e testes
|
‚îî‚îÄ‚îÄ src/                   # C√≥digo-fonte da aplica√ß√£o
|
‚îú‚îÄ‚îÄ ingestion/         # Scripts de coleta de dados (Scrapers)
|
‚îú‚îÄ‚îÄ transformation/    # Scripts ETL para o AWS Glue
|
‚îî‚îÄ‚îÄ dwh/               # Scripts SQL (Schema DDL, DML)

---

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

---

## üë®‚Äçüíª Autor

Feito por **[Luis Cesar Ferreira do Carmo]**

[![linkedin](https://img.shields.io/badge/linkedin-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/luis-cesar-ferreira-do-carmo/)
[![github](https://img.shields.io/badge/github-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/LuisFcarmohttps://github.com/LuisFcarmo)